{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mtool\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import nltk\n",
    "nltk.download('wordnet') # first-time use only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NY Times Best Seller List\n",
    "I chose 24 titles off the New York times Best Seller list. I then searched for the 1st and the 20th organic search results for the first book in the list and used those search results for the second question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AmazonBookList =  ['All the Ugly and Wonderful Things: A Novel',\n",
    "                   'The Tuscan Child',\n",
    "                   'Where the Crawdads Sing',\n",
    "                   'The Nightingale: A Novel',\n",
    "                   'The Goldfinch: A Novel (Pulitzer Prize for Fiction)',\n",
    "                   'The Life We Bury',\n",
    "                   'All the Light We Cannot See: A Novel',\n",
    "                   'Spilled Milk: Based on a true story',\n",
    "                   'What Alice Forgot',\n",
    "                   'The Flight Attendant: A Novel',\n",
    "                   'Winter Garden',\n",
    "                   \"The Storyteller's Secret: A Novel\",\n",
    "                   'Ordinary Grace: A Novel',\n",
    "                   'All the Ugly and Wonderful Things: A Novel',\n",
    "                   'It Ends with Us: A Novel',\n",
    "                   'The Shack: Where Tragedy Confronts Eternity',\n",
    "                   'Beneath a Scarlet Sky: A Novel',\n",
    "                   'Before We Were Yours: A Novel',\n",
    "                   'Small Great Things: A Novel',\n",
    "                   \"The Boy on the Wooden Box: How the Impossible Became Possible . . . on Schindler's List\",\n",
    "                   'A Man Called Ove: A Novel',\n",
    "                   \"The Ladies' Room\",\n",
    "                   'The Butterfly Garden (The Collector Book 1)',\n",
    "                   'HOSTILE WITNESS: A Josie Bates Thriller (The Witness Series Book 1)']\n",
    "\n",
    "stopword_list = ['a','the','and','novel','s']\n",
    "\n",
    "GoogleResults = [\"All the Ugly and Wonderful Things: A Novel\",\"All the Ugly and Wonderful Things by Bryn Greenwood - Goodreads\\\n",
    "All the Ugly and Wonderful Things book. Read 9840 reviews from the world's largest community for readers.\\\n",
    "As the daughter of a meth dealer, Wavy knows n..\",\"All The Ugly And Wonderful Things - By Bryn Greenwood (Hardcover\\\n",
    "A New York Times and USA Today bestseller - Book of the Month Club 2016 Book of the Year - Second Place Goodreads Best Fiction\\\n",
    "of 2016. A beautiful and\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code found online at to calculate cosine similarity from:\n",
    "https://sites.temple.edu/tudsc/2017/03/30/measuring-similarity-between-texts-in-python/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "     return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "     return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words=stopword_list)\n",
    "    \n",
    "def cos_similarity(textlist):\n",
    "    tfidf = TfidfVec.fit_transform(textlist)\n",
    "    return (tfidf * tfidf.T).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - Find the two titles that are most similar\n",
    "#### Calculate similarity matrix\n",
    "The matrix below has rows for each book, and the cosine similarity value for all books in the corpus. The matrix is arranged by *[book:book]* For example *[Row0:Col0]* = 1.  This is the cosine similarity for Book 1 against itself. If we look at *[Row0:Col6]*, the cosine similarity value for Book 1 and Book 7 is .16600436. We search this matrix to find the largest value that is not 1 and that corresponds to the books that are most similar.  Each value of 0 indicates titles that have no similarity, and in this list there are many books with no similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.         0.         0.         0.         0.\n",
      "  0.16600436 0.         0.         0.         0.         0.\n",
      "  0.         1.         0.         0.         0.         0.\n",
      "  0.23476561 0.         0.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.21465975 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         1.\n",
      "  0.17252475 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.20784557\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.16600436 0.         0.         0.         0.         0.17252475\n",
      "  1.         0.         0.         0.         0.         0.\n",
      "  0.         0.16600436 0.         0.         0.         0.14696901\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.18700265 0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.28122516 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [1.         0.         0.         0.         0.         0.\n",
      "  0.16600436 0.         0.         0.         0.         0.\n",
      "  0.         1.         0.         0.         0.         0.\n",
      "  0.23476561 0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.21465975 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.20784557\n",
      "  0.14696901 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.23476561 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.23476561 0.         0.         0.         0.\n",
      "  1.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.18700265 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.28122516 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         1.         0.23105876]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.23105876 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "titleMatrix = cos_similarity(AmazonBookList)\n",
    "print (titleMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the code belos we sort the array to find the largest value that is not 1. We'll then search the original matrix to find the *[Row:Col]* that corresponds to that number and those indicies will be the matching books.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Cosine Similarity Scores\n",
      "-------------------------------\n",
      "[0.         0.14696901 0.16600436 0.17252475 0.18700265 0.20784557\n",
      " 0.21465975 0.23105876 0.23476561 0.28122516 1.         1.\n",
      " 1.         1.        ]\n",
      "\n",
      "HighScore\n",
      "--------\n",
      "0.2812251571902398\n"
     ]
    }
   ],
   "source": [
    "titleArray = np.asarray(titleMatrix).reshape(-1)\n",
    "#titleArray = np.unique(titleArray.sort())\n",
    "uniquArray = np.unique(titleArray)\n",
    "print ('Unique Cosine Similarity Scores')\n",
    "print('-------------------------------')\n",
    "print (uniquArray)\n",
    "highScore = 0\n",
    "lowScore = 1\n",
    "for score in uniquArray:\n",
    "    if (score > highScore) and (score < .99 ): # Use .99 for rounding errors in cosine\n",
    "        highScore = score\n",
    "#    if (score < lowScore) and (score >0):\n",
    "#        lowScore = score\n",
    "print('\\nHighScore')\n",
    "print('--------')\n",
    "print(highScore)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now find the index with the high score and find the names of the books.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 22] [22 10]\n"
     ]
    }
   ],
   "source": [
    "book1, book2 = np.where(titleMatrix == highScore)\n",
    "print(book1,book2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winter Garden at index 10 is most similar to The Butterfly Garden (The Collector Book 1) at index 22 with a cosine similarity of 0.28123\n"
     ]
    }
   ],
   "source": [
    "print('{} at index {:2d} is most similar to {} at index {:2d} with a cosine similarity of {:1.5f}'.format(\n",
    "    AmazonBookList[book1[0]], book1[0], AmazonBookList[book1[1]], book1[1],highScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 \n",
    "#### Compare the two capsules from organic search with the book title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.42708791 0.22420408]\n",
      " [0.42708791 1.         0.25452838]\n",
      " [0.22420408 0.25452838 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "searchResultsMatrix = cos_similarity(GoogleResults)\n",
    "print (searchResultsMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Cosine Similarity Scores\n",
      "-------------------------------\n",
      "[0.22420408 0.25452838 0.42708791 1.         1.         1.        ]\n",
      "\n",
      "HighScore\n",
      "--------\n",
      "0.4270879126473651\n"
     ]
    }
   ],
   "source": [
    "searchArray = np.asarray(searchResultsMatrix).reshape(-1)\n",
    "#titleArray = np.unique(titleArray.sort())\n",
    "uniqueSearchArray = np.unique(searchArray)\n",
    "print ('Unique Cosine Similarity Scores')\n",
    "print('-------------------------------')\n",
    "print (uniqueSearchArray)\n",
    "highScore = 0\n",
    "lowScore = 1\n",
    "for score in uniqueSearchArray:\n",
    "    if (score > highScore) and (score < .99 ): # Use .99 for rounding errors in cosine\n",
    "        highScore = score\n",
    "#    if (score < lowScore) and (score >0):\n",
    "#        lowScore = score\n",
    "print('\\nHighScore')\n",
    "print('--------')\n",
    "print(highScore)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] [1 0]\n"
     ]
    }
   ],
   "source": [
    "text1, text2 = np.where(searchResultsMatrix == highScore)\n",
    "print(text1,text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text at index 1 has most in common with the title with a cosine similarity score of 0.42709\n"
     ]
    }
   ],
   "source": [
    "print('The text at index {} has most in common with the title with a cosine similarity score of {:1.5f}'.format(\n",
    "text1[1],highScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of findings\n",
    "The pairwise cosine comparison scores of the first 24 titles in the New York Times Best Seller list was not very high for most titles. The results of this comparison will change as the titles change, but this result is expected. Most of the titles are short and do not contain any words in common. The titles that had the largest similarity score both had the word *garden* in them so this result makes sense. Many of the titles copied from the Amazon website had a description of *A Novel* added to the end of the title name. This description is not part of the original title so the word *Novel* was cleaned from the text. If this word had been retained we no doubtedly would have seen a larger collection of non-zero scores.\n",
    "\n",
    "When comparing the 1st and the 20th organic web results, the first web result had a higher similariy score. This makes sense as the Google search algorithm should list hits with a higher similarity score before those with a lower similarity score. We can speculate that if we looked at all 20 organic results, the similarity scores would go down as we move down the list of search results. This assumes our similarity scoring is similar to the one used by Google.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
